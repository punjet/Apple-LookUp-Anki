{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX9xzLur4tus"
      },
      "source": [
        "# MDX-Net Colab\n",
        "<div style=\"display:flex; align-items:center; font-size: 16px;\">\n",
        "  <img src=\"https://github.githubassets.com/pinned-octocat.svg\" alt=\"icon1\" style=\"margin-right:10px; height: 20px;\" width=\"1.5%\">\n",
        "  <span>Trained models provided in this notebook are from <a href=\"https://github.com/Anjok07\">UVR-GUI</a>.</span>\n",
        "</div>\n",
        "<div style=\"display:flex; align-items:center; font-size: 16px;\">\n",
        "  <img src=\"https://github.com/Anjok07/ultimatevocalremovergui/raw/master/gui_data/img/GUI-Icon.ico\" alt=\"icon2\" style=\"margin-right:10px; height: 20px;margin-top:10px\" width=\"1.5%\">\n",
        "  <span>OFFICIAL UVR GITHUB PAGE: <a href=\"https://github.com/Anjok07/ultimatevocalremovergui\">here</a>.</span>\n",
        "</div>\n",
        "<div style=\"display:flex; align-items:center; font-size: 16px;\">\n",
        "  <img src=\"https://avatars.githubusercontent.com/u/24620594\" alt=\"icon3\" style=\"margin-right:10px; height: 20px;\" width=\"1.5%\">\n",
        "  <span>OFFICIAL CLI Version: <a href=\"https://github.com/tsurumeso/vocal-remover\">here</a>.</span>\n",
        "</div>\n",
        "<div style=\"display:flex; align-items:center; font-size: 16px;\">\n",
        "  <img src=\"https://icons.getbootstrap.com/assets/icons/discord.svg\" alt=\"icon4\" style=\"margin-right:10px; height: 20px;\" width=\"1.5%\">\n",
        "  <span>Join our <a href=\"https://cutt.ly/0TcDjmo\">Discord server</a>!</span>\n",
        "</div>\n",
        "<sup><br>Ultimate Vocal Remover (unofficial)</sup>\n",
        "<sup><br>MDX-Net by <a href=\"https://github.com/kuielab\">kuielab</a> and adapted for Colaboratory by <a href=\"https://www.youtube.com/channel/UC0NiSV1jLMH-9E09wiDVFYw\">AudioHacker</a>.</sup>\n",
        "\n",
        "<sup><br>Your support means a lot to me. If you enjoy my work, please consider buying me a ko-fi:<br></sup>\n",
        "[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/X8X6M8FR0)<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common issues\n",
        "\n",
        "- For \"OSError: [Errno 107] Transport endpoint is not connected\", go to Environment and launch session again, then rerun that cell.<br>\n",
        "- For \"credential propagation unsuccessful\" error, go to Environment, kill the environment, wait 30-40s (or potentially click refresh button in the file manager) and start over. <br>\n",
        "- If you exceed GPU Colab limit for free users, switch Google account in the right corner, and use the same account for GDrive. <br>\n",
        "- Grant all the privileges for GDrive, or you'll see mounting error.<br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aywwSSWivQy_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3J69RV7G8ocb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddfe59a-d71b-46e8-cbcc-0b36762a495a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy 1.26.0 is already installed.\n",
            "/content\n",
            "Setting up... Please wait around 1-2 minute(s).\n",
            "Ошибка загрузки названий. Ответ сервера: <!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initi\n",
            "Looking in indexes: https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ort-cuda-12-nightly/pypi/simple/\n",
            "Collecting ort-nightly-gpu\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/d3daa2b0-aa56-45ac-8145-2c3dc0661c87/pypi/download/ort-nightly-gpu/1.19.dev20240531001/ort_nightly_gpu-1.19.0.dev20240531001-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (202.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.4/202.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from ort-nightly-gpu)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/d3daa2b0-aa56-45ac-8145-2c3dc0661c87/pypi/download/coloredlogs/15.0.1/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from ort-nightly-gpu) (25.12.19)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from ort-nightly-gpu) (1.26.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ort-nightly-gpu) (26.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from ort-nightly-gpu) (5.29.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from ort-nightly-gpu) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->ort-nightly-gpu)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/d3daa2b0-aa56-45ac-8145-2c3dc0661c87/pypi/download/humanfriendly/10/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->ort-nightly-gpu) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, ort-nightly-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 ort-nightly-gpu-1.19.0.dev20240531001\n",
            "Collecting onnxruntime-gpu==1.21.0\n",
            "  Downloading onnxruntime_gpu-1.21.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.21.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.21.0) (25.12.19)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.21.0) (1.26.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.21.0) (26.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.21.0) (5.29.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.21.0) (1.14.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime-gpu==1.21.0) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu==1.21.0) (1.3.0)\n",
            "Downloading onnxruntime_gpu-1.21.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (280.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/280.8 MB\u001b[0m \u001b[31m163.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
            "    data: bytes = self.__fp.read(amt)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 479, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1103, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
            "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
            "    self._complete_partial_requirements(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
            "    for link, (filepath, _) in batch_download:\n",
            "                               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
            "    for chunk in chunks:\n",
            "                 ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
            "\u001b[0minstalling requirements... done\n",
            "Please log in to your account by following the prompts in the pop-up tab.\n",
            "This step is necessary to install the files to your Google Drive.\n",
            "If you have any concerns about the safety of this notebook, you can choose not to mount your drive by unchecking the \"MountDrive\" checkbox.\n",
            "Mounted at /content/drive\n",
            "done!\n",
            "Importing required libraries... finished setting up!\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### Initialization\n",
        "import importlib.metadata\n",
        "\n",
        "desired_version = \"1.26.0\"\n",
        "\n",
        "try:\n",
        "    installed_version = importlib.metadata.version(\"numpy\")\n",
        "    if installed_version == desired_version:\n",
        "        print(f\"NumPy {desired_version} is already installed.\")\n",
        "    else:\n",
        "        print(f\"Installing NumPy {desired_version} (current: {installed_version})...\")\n",
        "        !pip install numpy=={desired_version} --prefer-binary\n",
        "        import os\n",
        "        os._exit(00)  # Restart runtime for changes to take effect\n",
        "except importlib.metadata.PackageNotFoundError:\n",
        "    print(f\"NumPy is not installed. Installing {desired_version}...\")\n",
        "    !pip install numpy=={desired_version} --prefer-binary\n",
        "    import os\n",
        "    os._exit(00)\n",
        "\n",
        "%cd /content\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import gc\n",
        "import psutil\n",
        "import requests\n",
        "import subprocess\n",
        "import glob\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive, files, output\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "if \"first_cell_ran\" in locals():\n",
        "    print(\"You've ran this cell for this session. No need to run it again.\\nif you think something went wrong or you want to change mounting path, restart the runtime.\")\n",
        "else:\n",
        "    print('Setting up... Please wait around 1-2 minute(s).')\n",
        "\n",
        "    branch = \"https://github.com/NaJeongMo/Colab-for-MDX_B\"\n",
        "\n",
        "    model_params = \"https://raw.githubusercontent.com/TRvlvr/application_data/main/mdx_model_data/model_data_new.json\"\n",
        "    _Models = \"https://github.com/TRvlvr/model_repo/releases/download/all_public_uvr_models/\"\n",
        "    # _models = \"https://pastebin.com/raw/jBzYB8vz\"\n",
        "    _models = \"https://raw.githubusercontent.com/TRvlvr/application_data/main/filelists/download_checks.json\"\n",
        "    stem_naming = \"https://pastebin.com/raw/mpH4hRcF\"\n",
        "    arl_check_endpoint = 'https://dz.doubledouble.top/check' # param: arl?=<>\n",
        "\n",
        "    file_folder = \"Colab-for-MDX_B\"\n",
        "\n",
        "    try:\n",
        "        req1 = requests.get(_models)\n",
        "        model_ids = req1.json()[\"mdx_download_list\"].values()\n",
        "    except Exception:\n",
        "        print(f\"Ошибка загрузки моделей. Ответ сервера: {req1.text[:100]}\")\n",
        "        model_ids = []\n",
        "\n",
        "    try:\n",
        "        req2 = requests.get(model_params)\n",
        "        model_params = req2.json()\n",
        "    except Exception:\n",
        "        print(f\"Ошибка загрузки параметров. Ответ сервера: {req2.text[:100]}\")\n",
        "        model_params = {}\n",
        "\n",
        "    try:\n",
        "        req3 = requests.get(stem_naming)\n",
        "        stem_naming = req3.json()\n",
        "    except Exception:\n",
        "        print(f\"Ошибка загрузки названий. Ответ сервера: {req3.text[:100]}\")\n",
        "        stem_naming = {}\n",
        "\n",
        "    os.makedirs(\"tmp_models\", exist_ok=True)\n",
        "    # @markdown Start the cell below, and when Numpy has been installed, run that cell again (so when you'll see the session crash info) - do that on every first Colab launch from now now to finish the Colab initialization, then continue below.  <br> <br>If you don't wish to mount Google Drive, uncheck this box.\n",
        "    MountDrive = True  # @param{type:\"boolean\"}\n",
        "    # @markdown The path for the drive to be mounted: Please be cautious when modifying this as it can cause issues if not done properly.\n",
        "    mounting_path = \"/content/drive/MyDrive\"  # @param [\"snippets:\",\"/content/drive/MyDrive\",\"/content/drive/Shareddrives/<your shared drive name>\", \"/content/drive/Shareddrives/Shared Drive\"]{allow-input: true}\n",
        "    # @markdown Force update and disregard local changes: discards all local modifications in your repository, effectively replacing all files with the versions from the original commit.\n",
        "    force_update = False  # @param{type:\"boolean\"}\n",
        "    # @markdown Auto Update (does not discard your changes)\n",
        "    auto_update = True  # @param{type:\"boolean\"}\n",
        "\n",
        "    #testing\n",
        "    #!python -m pip install numpy==1.26\n",
        "    #!pip install numpy==1.25.2\n",
        "    #!pip install torch==1.13.1\n",
        "    #!python -m pip install scipy==1.13.1\n",
        "\n",
        "    #onnxruntime-gpu fix\n",
        "    !python -m pip install ort-nightly-gpu --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ort-cuda-12-nightly/pypi/simple/\n",
        "    !pip install onnxruntime-gpu==1.21.0\n",
        "\n",
        "    reqs_apt = []  # !sudo apt-get install\n",
        "    reqs_pip = [\"librosa\", \"deemix\", \"yt_dlp\"]  # pip3 install\n",
        "\n",
        "    class hide_opt:  # hide outputs\n",
        "        def __enter__(self):\n",
        "            self._original_stdout = sys.stdout\n",
        "            sys.stdout = open(os.devnull, \"w\")\n",
        "\n",
        "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "            sys.stdout.close()\n",
        "            sys.stdout = self._original_stdout\n",
        "\n",
        "    def get_size(bytes, suffix=\"B\"):  # read ram\n",
        "        global svmem\n",
        "        factor = 1024\n",
        "        for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "            if bytes < factor:\n",
        "                return f\"{bytes:.2f}{unit}{suffix}\"\n",
        "            bytes /= factor\n",
        "        svmem = psutil.virtual_memory()\n",
        "\n",
        "\n",
        "    print('installing requirements...',end=' ')\n",
        "    with hide_opt():\n",
        "        for x in reqs_apt:\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"install\", x])\n",
        "        for x in reqs_pip:\n",
        "            subprocess.run([\"python3\", \"-m\", \"pip\", \"install\", x])\n",
        "    print('done')\n",
        "\n",
        "    def install_or_mount_drive():\n",
        "        print(\n",
        "            \"Please log in to your account by following the prompts in the pop-up tab.\\nThis step is necessary to install the files to your Google Drive.\\nIf you have any concerns about the safety of this notebook, you can choose not to mount your drive by unchecking the \\\"MountDrive\\\" checkbox.\"\n",
        "        )\n",
        "        drive.mount(\"/content/drive\", force_remount=True)\n",
        "        os.chdir(mounting_path)\n",
        "        # check if previous installation is done\n",
        "        if os.path.exists(os.path.join(mounting_path, file_folder)):\n",
        "            # update checking\n",
        "            os.chdir(file_folder)\n",
        "\n",
        "            if force_update:\n",
        "                print('Force updating...')\n",
        "\n",
        "                commands = [\n",
        "                    [\"git\", \"pull\"],\n",
        "                    [\"git\", \"checkout\", \"--\", \".\"],\n",
        "                ]\n",
        "\n",
        "                for cmd in commands:\n",
        "                    subprocess.run(cmd)\n",
        "\n",
        "            elif auto_update:\n",
        "                print('Checking for updates...')\n",
        "                commands = [\n",
        "                    [\"git\", \"pull\"],\n",
        "                ]\n",
        "\n",
        "                for cmd in commands:\n",
        "                    subprocess.run(cmd)\n",
        "        else:\n",
        "            subprocess.run([\"git\", \"clone\", \"https://github.com/NaJeongMo/Colab-for-MDX_B.git\"])\n",
        "            os.chdir(file_folder)\n",
        "\n",
        "    def use_uvr_without_saving():\n",
        "        global mounting_path\n",
        "        print(\"Notice: files won't be saved to personal drive.\")\n",
        "        print(f\"Downloading {file_folder}...\", end=\" \")\n",
        "        mounting_path = \"/content\"\n",
        "        with hide_opt():\n",
        "            os.chdir(mounting_path)\n",
        "            subprocess.run([\"git\", \"clone\", \"https://github.com/NaJeongMo/Colab-for-MDX_B.git\"])\n",
        "            os.chdir(file_folder)\n",
        "\n",
        "    if MountDrive:\n",
        "        install_or_mount_drive()\n",
        "    else:\n",
        "        use_uvr_without_saving()\n",
        "    print(\"done!\")\n",
        "    if not os.path.exists(\"tracks\"):\n",
        "        os.mkdir(\"tracks\")\n",
        "\n",
        "    print('Importing required libraries...',end=' ')\n",
        "\n",
        "    import os\n",
        "    import mdx\n",
        "    import librosa\n",
        "    import torch\n",
        "    import soundfile as sf\n",
        "    import numpy as np\n",
        "    import yt_dlp\n",
        "\n",
        "    from deezer import Deezer\n",
        "    from deezer import TrackFormats\n",
        "    import deemix\n",
        "    from deemix.settings import load as loadSettings\n",
        "    from deemix.downloader import Downloader\n",
        "    from deemix import generateDownloadObject\n",
        "\n",
        "    logger = logging.getLogger(\"yt_dlp\")\n",
        "    logger.setLevel(logging.ERROR)\n",
        "\n",
        "    def id_to_ptm(mkey):\n",
        "        if mkey in model_ids:\n",
        "            mpath = f\"/content/tmp_models/{mkey}\"\n",
        "            if not os.path.exists(f'/content/tmp_models/{mkey}'):\n",
        "                print('Downloading model...',end=' ')\n",
        "                subprocess.run(\n",
        "                    [\"wget\", _Models+mkey, \"-O\", mpath]\n",
        "                )\n",
        "                print(f'saved to {mpath}')\n",
        "                # get_ipython().system(f'gdown {model_id} -O /content/tmp_models/{mkey}')\n",
        "                return mpath\n",
        "            else:\n",
        "                return mpath\n",
        "        else:\n",
        "            mpath = f'models/{mkey}'\n",
        "            return mpath\n",
        "\n",
        "    def prepare_mdx(custom_param=False, dim_f=None, dim_t=None, n_fft=None, stem_name=None, compensation=None):\n",
        "        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        if custom_param:\n",
        "            assert not (dim_f is None or dim_t is None or n_fft is None or compensation is None), 'Custom parameter selected, but incomplete parameters are provided.'\n",
        "            mdx_model = mdx.MDX_Model(\n",
        "                device,\n",
        "                dim_f = dim_f,\n",
        "                dim_t = dim_t,\n",
        "                n_fft = n_fft,\n",
        "                stem_name=stem_name,\n",
        "                compensation=compensation\n",
        "            )\n",
        "        else:\n",
        "            model_hash = mdx.MDX.get_hash(onnx)\n",
        "            if model_hash in model_params:\n",
        "                mp = model_params.get(model_hash)\n",
        "                mdx_model = mdx.MDX_Model(\n",
        "                    device,\n",
        "                    dim_f = mp[\"mdx_dim_f_set\"],\n",
        "                    dim_t = 2**mp[\"mdx_dim_t_set\"],\n",
        "                    n_fft = mp[\"mdx_n_fft_scale_set\"],\n",
        "                    stem_name=mp[\"primary_stem\"],\n",
        "                    compensation=compensation if not custom_param and compensation is not None else mp[\"compensate\"]\n",
        "                )\n",
        "        return mdx_model\n",
        "\n",
        "    def run_mdx(onnx, mdx_model,filename,diff=False,suffix=None,diff_suffix=None, denoise=False, m_threads=1):\n",
        "        mdx_sess = mdx.MDX(onnx,mdx_model)\n",
        "        print(f\"Processing: {filename}\")\n",
        "        wave, sr = librosa.load(filename,mono=False, sr=44100)\n",
        "        # normalizing input wave gives better output\n",
        "        peak = max(np.max(wave), abs(np.min(wave)))\n",
        "        wave /= peak\n",
        "        if denoise:\n",
        "            wave_processed = -(mdx_sess.process_wave(-wave, m_threads)) + (mdx_sess.process_wave(wave, m_threads))\n",
        "            wave_processed *= 0.5\n",
        "        else:\n",
        "            wave_processed = mdx_sess.process_wave(wave, m_threads)\n",
        "        # return to previous peak\n",
        "        wave_processed *= peak\n",
        "\n",
        "        stem_name = mdx_model.stem_name if suffix is None else suffix # use suffix if provided\n",
        "        save_path = f\"{os.path.basename(os.path.splitext(filename)[0])}_{stem_name}.wav\"\n",
        "        save_path = os.path.join(\n",
        "                'separated',\n",
        "                save_path\n",
        "            )\n",
        "        sf.write(\n",
        "            save_path,\n",
        "            wave_processed.T,\n",
        "            sr\n",
        "        )\n",
        "\n",
        "        print(f'done, saved to: {save_path}')\n",
        "\n",
        "        if diff:\n",
        "            diff_stem_name = stem_naming.get(stem_name) if diff_suffix is None else diff_suffix # use suffix if provided\n",
        "            stem_name = f\"{stem_name}_diff\" if diff_stem_name is None else diff_stem_name\n",
        "            save_path = f\"{os.path.basename(os.path.splitext(filename)[0])}_{stem_name}.wav\"\n",
        "            save_path = os.path.join(\n",
        "                    'separated',\n",
        "                    save_path\n",
        "                )\n",
        "            sf.write(\n",
        "                save_path,\n",
        "                (-wave_processed.T*mdx_model.compensation)+wave.T,\n",
        "                sr\n",
        "            )\n",
        "            print(f'invert done, saved to: {save_path}')\n",
        "        del mdx_sess, wave_processed, wave\n",
        "        gc.collect()\n",
        "\n",
        "    def is_valid_url(url):\n",
        "        import re\n",
        "        regex = re.compile(\n",
        "            r'^https?://'\n",
        "            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+[A-Z]{2,6}\\.?|'\n",
        "            r'localhost|'\n",
        "            r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'\n",
        "            r'(?::\\d+)?'\n",
        "            r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
        "        return url is not None and regex.search(url)\n",
        "\n",
        "    def download_deezer(link, arl, fmt='FLAC'):\n",
        "        match fmt:\n",
        "            case 'FLAC':\n",
        "                bitrate = TrackFormats.FLAC\n",
        "            case 'MP3_320':\n",
        "                bitrate = TrackFormats.MP3_320\n",
        "            case 'MP3_128':\n",
        "                bitrate = TrackFormats.MP3_128\n",
        "            case _:\n",
        "                bitrate = TrackFormats.MP3_128\n",
        "\n",
        "        dz = Deezer()\n",
        "        settings = loadSettings('dz_config')\n",
        "        settings['downloadLocation'] = './tracks'\n",
        "        if not dz.login_via_arl(arl.strip()):\n",
        "            raise Exception('Error while logging in with provided ARL.')\n",
        "        downloadObject = generateDownloadObject(dz, link, bitrate)\n",
        "        print(f'Downloading {downloadObject.type}: \"{downloadObject.title}\" by {downloadObject.artist}...',end=' ',flush=True)\n",
        "        Downloader(dz, downloadObject, settings).start()\n",
        "        print(f'done.')\n",
        "\n",
        "        path_to_audio = []\n",
        "        for file in downloadObject.files:\n",
        "            path_to_audio.append(file[\"path\"])\n",
        "\n",
        "        return path_to_audio\n",
        "\n",
        "    def download_link(url):\n",
        "        ydl_opts = {\n",
        "            'format': 'bestvideo+bestaudio/best',\n",
        "            'outtmpl': '%(title)s.%(ext)s',\n",
        "            'nocheckcertificate': True,\n",
        "            'ignoreerrors': True,\n",
        "            'no_warnings': True,\n",
        "            'extractaudio': True,\n",
        "        }\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            result = ydl.extract_info(url, download=True)\n",
        "            download_path = ydl.prepare_filename(result)\n",
        "        return download_path\n",
        "\n",
        "    print('finished setting up!')\n",
        "    first_cell_ran = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y onnxruntime-gpu ort-nightly-gpu\n",
        "!pip install onnxruntime-gpu"
      ],
      "metadata": {
        "id": "Os33M1EQMBDk",
        "outputId": "96a87e89-3285-47b9-fa77-ff1012748049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping onnxruntime-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: ort-nightly-gpu 1.19.0.dev20240531001\n",
            "Uninstalling ort-nightly-gpu-1.19.0.dev20240531001:\n",
            "  Successfully uninstalled ort-nightly-gpu-1.19.0.dev20240531001\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.24.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.12.19)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.26.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (26.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (5.29.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.14.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnxruntime_gpu-1.24.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (252.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-gpu\n",
            "Successfully installed onnxruntime-gpu-1.24.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4hd1TzEGCiRo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "if 'first_cell_ran' in locals():\n",
        "    os.chdir(mounting_path + '/' + file_folder + '/')\n",
        "    #parameter markdowns-----------------\n",
        "    #@markdown ### Input files\n",
        "    #@markdown Track filename: Upload your songs to the \"tracks\" folder (by default in MyDrive\\Colab-for-MDX_B\\). <br>You may provide multiple links/files by spliting them with ; <br>Or you can paste direct file path from file manager on the left.\n",
        "    filename = \"/content/drive/MyDrive/UVR/Goblin_Stereo.wav\" #@param {type:\"string\"}\n",
        "    #@markdown onnx model (if you have your own model, upload it in models folder)\n",
        "    onnx = \"Kim_Vocal_2.onnx\" #@param [\"Kim_Inst.onnx\", \"Kim_Vocal_1.onnx\", \"Kim_Vocal_2.onnx\", \"kuielab_a_bass.onnx\", \"kuielab_a_drums.onnx\", \"kuielab_a_other.onnx\", \"kuielab_a_vocals.onnx\", \"kuielab_b_bass.onnx\", \"kuielab_b_drums.onnx\", \"kuielab_b_other.onnx\", \"kuielab_b_vocals.onnx\", \"Reverb_HQ_By_FoxJoy.onnx\", \"UVR-MDX-NET-Inst_1.onnx\", \"UVR-MDX-NET-Inst_2.onnx\", \"UVR-MDX-NET-Inst_3.onnx\", \"UVR-MDX-NET-Inst_HQ_1.onnx\", \"UVR-MDX-NET-Inst_HQ_2.onnx\", \"UVR-MDX-NET-Inst_Main.onnx\", \"UVR_MDXNET_1_9703.onnx\", \"UVR_MDXNET_2_9682.onnx\", \"UVR_MDXNET_3_9662.onnx\", \"UVR_MDXNET_9482.onnx\", \"UVR_MDXNET_KARA.onnx\", \"UVR_MDXNET_KARA_2.onnx\", \"UVR_MDXNET_Main.onnx\", \"UVR-MDX-NET-Inst_HQ_3.onnx\", \"UVR-MDX-NET-Inst_HQ_4.onnx\", \"UVR-MDX-NET-Inst_HQ_5.onnx\", \"UVR-MDX-NET-Voc_FT.onnx\", \"UVR-MDX-NET_Crowd_HQ_1.onnx\"]{allow-input: true}\n",
        "    #@markdown process all: processes all tracks inside tracks/ folder instead. (filename will be ignored!)\n",
        "    process_all = False  # @param{type:\"boolean\"}\n",
        "\n",
        "\n",
        "    #@markdown ### Settings\n",
        "    #@markdown invert: get difference between input and output (e.g get Instrumental out of Vocal model or in reverse)\n",
        "    invert = False  # @param{type:\"boolean\"}\n",
        "    #@markdown denoise: get rid of MDX noise. (This processes input track twice)\n",
        "    denoise = True  # @param{type:\"boolean\"}\n",
        "    #@markdown m_threads: like batch size, processes input wave in n threads. (beneficial for CPU)\n",
        "    m_threads = 1 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "\n",
        "    #@markdown ### Custom model parameters (Only use this if you're using new/unofficial/custom models)\n",
        "    #@markdown Use custom model parameters. (Default: unchecked, or auto)\n",
        "    use_custom_parameter = False  # @param{type:\"boolean\"}\n",
        "    #@markdown Output file suffix (usually the stem name e.g Vocals)\n",
        "    suffix = \"Vocals_custom\" #@param [\"Vocals\", \"Drums\", \"Bass\", \"Other\"]{allow-input: true}\n",
        "    suffix_invert = \"Instrumental_custom\" #@param [\"Instrumental\", \"Drumless\", \"Bassless\", \"Instruments\"]{allow-input: true}\n",
        "    #@markdown Model parameters\n",
        "    dim_f = 3072 #@param {type: \"integer\"}\n",
        "    dim_t = 256 #@param {type: \"integer\"}\n",
        "    n_fft = 6144 #@param {type: \"integer\"}\n",
        "    #@markdown use custom compensation: only if you have your own compensation value for your model. this still apply even if you don't have use_custom_parameter checked (Default: unchecked, or auto)\n",
        "    use_custom_compensation = False  # @param{type:\"boolean\"}\n",
        "    compensation = 1.000 #@param {type: \"number\"}\n",
        "\n",
        "    #@markdown ### Extras\n",
        "    #@markdown Deezer arl: paste your ARL here for Deezer tracks directly!\n",
        "    arl = \"\" #@param {type:\"string\"}\n",
        "    #@markdown Track format: select track quality/format\n",
        "    track_format = \"FLAC\" #@param [\"FLAC\",\"MP3_320\",\"MP3_128\"]\n",
        "    #@markdown Print settings being used in the run\n",
        "    print_settings = True  # @param{type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "    onnx = id_to_ptm(onnx)\n",
        "    compensation = compensation if use_custom_compensation or use_custom_parameter else None\n",
        "    mdx_model = prepare_mdx(use_custom_parameter, dim_f, dim_t, n_fft, compensation=compensation)\n",
        "\n",
        "    filename_split = filename.split(';')\n",
        "\n",
        "    usable_files = []\n",
        "\n",
        "    if not process_all:\n",
        "        for fn in filename_split:\n",
        "            fn = fn.strip()\n",
        "            if is_valid_url(fn):\n",
        "                dm, ltype, lid = deemix.parseLink(fn)\n",
        "                if ltype and lid:\n",
        "                    usable_files += download_deezer(fn, arl, track_format)\n",
        "                else:\n",
        "                    print('downloading link...',end=' ')\n",
        "                    usable_files+=[download_link(fn)]\n",
        "                    print('done')\n",
        "            else:\n",
        "                usable_files.append(os.path.join('tracks',fn))\n",
        "    else:\n",
        "        for fn in glob.glob('tracks/*'):\n",
        "            usable_files.append(fn)\n",
        "    for filename in usable_files:\n",
        "        suffix_naming = suffix if use_custom_parameter else None\n",
        "        diff_suffix_naming = suffix_invert if use_custom_parameter else None\n",
        "        run_mdx(onnx, mdx_model, filename, diff=invert,suffix=suffix_naming,diff_suffix=diff_suffix_naming,denoise=denoise)\n",
        "\n",
        "    if print_settings:\n",
        "        print()\n",
        "        print('[MDX-Net_Colab settings used]')\n",
        "        print(f'Model used: {onnx}')\n",
        "        print(f'Model MD5: {mdx.MDX.get_hash(onnx)}')\n",
        "        print(f'Using de-noise: {denoise}')\n",
        "        print(f'Model parameters:')\n",
        "        print(f'    -dim_f: {mdx_model.dim_f}')\n",
        "        print(f'    -dim_t: {mdx_model.dim_t}')\n",
        "        print(f'    -n_fft: {mdx_model.n_fft}')\n",
        "        print(f'    -compensation: {mdx_model.compensation}')\n",
        "        print()\n",
        "        print('[Input file]')\n",
        "        print('filename(s): ')\n",
        "        for filename in usable_files:\n",
        "            print(f'    -{filename}')\n",
        "\n",
        "    del mdx_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMVwX5RhZSRP"
      },
      "source": [
        "# Guide\n",
        "\n",
        "This tutorial guide will walk you through the steps to use the features of this Colab notebook.\n",
        "\n",
        "## Mount Drive\n",
        "\n",
        "To mount your Google Drive, follow these steps:\n",
        "\n",
        "1. Check the box next to \"MountDrive\" if you want to mount Google Drive.\n",
        "2. Modify the \"mounting_path\" if you want to specify a different path for the drive to be mounted. **Note:** Be cautious when modifying this path as it can cause issues if not done properly.\n",
        "3. Check the box next to \"Force update and disregard local changes\" if you want to discard all local modifications in your repository and replace the files with the versions from the original commit.\n",
        "4. Check the box next to \"Auto Update\" if you want to automatically update without discarding your changes. Leave it unchecked if you want to manually update.\n",
        "\n",
        "## Input Files\n",
        "\n",
        "To upload your songs, follow these steps:\n",
        "\n",
        "1. Specify the \"track filename\" for your songs. You can provide multiple links or files by separating them with a semicolon (;).\n",
        "2. Upload your songs to the \"tracks\" folder.\n",
        "\n",
        "## ONNX Model\n",
        "\n",
        "If you have your own ONNX model, follow these steps:\n",
        "\n",
        "1. Upload your model to the \"models\" folder.\n",
        "2. Specify the \"onnx\" filename for your model.\n",
        "\n",
        "## Processing\n",
        "\n",
        "To process your tracks, follow these steps:\n",
        "\n",
        "1. If you want to process all tracks inside the \"tracks\" folder, check the box next to \"process_all\" and ignore the \"filename\" field.\n",
        "2. Specify any additional settings you want:\n",
        "   - Check the box next to \"invert\" to get the difference between input and output (e.g., get Instrumental out of Vocals).\n",
        "   - Check the box next to \"denoise\" to get rid of MDX noise. This processes the input track twice.\n",
        "   - Specify custom model parameters only if you're using new/unofficial/custom models. Use the \"use_custom_parameter\" checkbox to enable this feature.\n",
        "   - Specify the output file suffix, which is usually the stem name (e.g., Vocals). Use the \"suffix\" field to specify the suffix for normal processing and the \"suffix_invert\" field for inverted processing.\n",
        "\n",
        "## Model Parameters\n",
        "\n",
        "Specify the following custom model parameters if applicable:\n",
        "\n",
        "- \"dim_f\": The value for the `dim_f` parameter.\n",
        "- \"dim_t\": The value for the `dim_t` parameter.\n",
        "- \"n_fft\": The value for the `n_fft` parameter.\n",
        "- Check the box next to \"use_custom_compensation\" if you have your own compensation value for your model. Specify the compensation value in the \"compensation\" field.\n",
        "\n",
        "## Extras\n",
        "\n",
        "If you're working with Deezer tracks, paste your ARL (Authentication Request Library) in the \"arl\" field to directly access the tracks.\n",
        "\n",
        "Specify the \"Track format\" by selecting the desired quality/format for the track.\n",
        "\n",
        "To print the settings being used in the run, check the box next to \"print_settings\".\n",
        "\n",
        "That's it! You're now ready to use this Colab notebook. Enjoy!\n",
        "\n",
        "## For more detailed guide, proceed to this <a href=\"https://docs.google.com/document/d/17fjNvJzj8ZGSer7c7OFe_CNfUKbAxEh_OBv94ZdRG5c\">link</a>.\n",
        "credits: (discord) deton24"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}